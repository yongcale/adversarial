
# Reference List

## Adversarial Example/Training
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
* [Synthesizing Robust Adversarial Examples](https://arxiv.org/abs/1707.07397)
* [Standard implementation of attacks, for adversarial training and reproducible benchmarks](https://github.com/tensorflow/cleverhans)
* [Practical black-box attacks against machine learning](https://arxiv.org/abs/1602.02697) 
* [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) 
* [Delving into Transferable Adversarial Examples and Black-box attacks](https://arxiv.org/abs/1611.02770) 
* [Adversarial Examples for Semantic Image Segmentation](https://arxiv.org/abs/1703.01101) 
* [Adversarial Examples for Semantic Segmentation and Object Detection](https://arxiv.org/pdf/1703.08603.pdf) 
* [Universal adversarial perturbations](https://arxiv.org/abs/1610.08401) 
* [The Limitations of Deep Learning in Adversarial Settings](https://arxiv.org/abs/1511.07528)
* [Deepfool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)
* [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images](https://arxiv.org/abs/1412.1897)

---

* [Adversarial examples in the physical world.](https://arxiv.org/abs/1607.02533) 
* [Delving into adversarial attacks on deep policies](https://arxiv.org/abs/1705.06452) 
* [Learning to Protect Communications with Adversarial Neural Cryptography](https://arxiv.org/abs/1610.06918v1)
* [Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284)  (http://rll.berkeley.edu/adversarial/) 
* [Ensemble Adversarial Training: Attacks and Defenses](https://arxiv.org/abs/1705.07204)
* [SafetyNet: Detecting and Rejecting Adversarial Examples Robustly](https://arxiv.org/abs/1704.00103)
* [Tactics of Adversarial Attack on Deep Reinforcement Learning Agents](https://arxiv.org/abs/1703.06748)
* [Detecting Adversarial Samples from Artifacts](https://arxiv.org/abs/1703.00410)
* [Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong](https://arxiv.org/abs/1706.04701)
---

## Good Resources
* [Generating Adversarial Examples for Speech Recognition](http://web.stanford.edu/class/cs224s/reports/Dan_Iter.pdf)
* [Breaking Linear Classifiers on ImageNet](http://karpathy.github.io/2015/03/30/breaking-convnets/) 
---


## Kaggle competition:
* https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack
* https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack
* https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack

---

## Blog on OpenAI:
* https://blog.openai.com/adversarial-example-research/
* https://blog.openai.com/robust-adversarial-inputs/

---


##  GAN:  (https://github.com/hindupuravinash/the-gan-zoo)
* [AE-GAN: adversarial eliminating with GAN](https://arxiv.org/abs/1707.05474)
* [Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN](https://arxiv.org/abs/1702.05983v1)
* [Generative Adversarial Networks](https://arxiv.org/abs/1406.266)

### GAN with NLP
* https://arxiv.org/abs/1711.04903
* https://arxiv.org/abs/1606.01614
* https://arxiv.org/abs/1711.10122

### visual Tracking with GAN
https://arxiv.org/pdf/1711.06564.pdf

---


